{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Econometric Game__: Effect of electoral accountability on corruption?\n",
    "   \n",
    "### Team 3: Katheryn Ding, Amber Wei, Max Ye\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.special import expit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uf', 'nsorteio', 'totrecursos', 'tot_os', 'pop', 'purb',\n",
       "       'p_secundario', 'cod_ibge6', 'pib_capita_02', 'op_01_04',\n",
       "       ...\n",
       "       'uf_d18', 'uf_d19', 'uf_d20', 'uf_d21', 'uf_d22', 'uf_d23', 'uf_d24',\n",
       "       'uf_d25', 'uf_d26', 'esample2'],\n",
       "      dtype='object', length=116)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corruption_df = pd.read_stata(\"corruptiondata.dta\")\n",
    "corruption_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mayor Covariates: ['pref_idade_tse', 'pref_masc', 'pref_escola', 'winmargin2000', 'exp_prefeito', 'party_d1', 'party_d3', 'party_d4', 'party_d5', 'party_d6', 'party_d7', 'party_d8', 'party_d9', 'party_d10', 'party_d11', 'party_d12', 'party_d13', 'party_d14', 'party_d15', 'party_d16', 'party_d17', 'party_d18']\n",
      "Municipal Covariates: ['lpop', 'purb', 'p_secundario', 'mun_novo', 'lpib02', 'gini_ipea']\n",
      "Political and Judicial Covariates: ['ENEP2000', 'ENLP2000', 'p_cad_pref']\n",
      "Dummy Covariates: ['sorteio1', 'sorteio2', 'sorteio3', 'sorteio4', 'sorteio5', 'sorteio6', 'sorteio7', 'sorteio8', 'sorteio9', 'sorteio10', 'uf_d1', 'uf_d2', 'uf_d3', 'uf_d4', 'uf_d5', 'uf_d6', 'uf_d7', 'uf_d8', 'uf_d9', 'uf_d10', 'uf_d11', 'uf_d12', 'uf_d13', 'uf_d14', 'uf_d15', 'uf_d16', 'uf_d17', 'uf_d18', 'uf_d19', 'uf_d20', 'uf_d21', 'uf_d22', 'uf_d23', 'uf_d24', 'uf_d25', 'uf_d26']\n"
     ]
    }
   ],
   "source": [
    "# Define covariates for each category\n",
    "\n",
    "# Mayor characteristics\n",
    "mayor_covariates = [\n",
    "    \"pref_idade_tse\",  # Age\n",
    "    \"pref_masc\",       # Gender\n",
    "    \"pref_escola\",     # Schooling\n",
    "    \"winmargin2000\",   # Margin of victory in 2000\n",
    "    \"exp_prefeito\"     # Was previously a mayor in a consecutive term\n",
    "] + [col for col in corruption_df.columns if col.startswith(\"party_d\")]\n",
    "\n",
    "# Municipal characteristics\n",
    "municipal_covariates = [\n",
    "    \"lpop\",           # Log of population in 2000\n",
    "    \"purb\",           # Percentage of population in urban sectors\n",
    "    \"p_secundario\",   # Percentage with at least secondary education\n",
    "    \"mun_novo\",       # New municipality indicator\n",
    "    \"lpib02\",         # Log of GDP per capita in 2002\n",
    "    \"gini_ipea\"       # Gini coefficient\n",
    "]\n",
    "\n",
    "# Political and Judicial characteristics\n",
    "political_judicial_covariates = [\n",
    "    \"ENEP2000\",  # Effective number of parties in 2000 mayor elections\n",
    "    \"ENLP2000\",  # Effective number of parties in 2000 legislative elections\n",
    "    \"p_cad_pref\" # Proportion of legislators from the same party as the mayor\n",
    "]\n",
    "\n",
    "# Dummies\n",
    "dummy_covariates = [\n",
    "    col for col in corruption_df.columns if col.startswith(\"uf_d\") or col.startswith(\"sorteio\")\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Mayor Covariates:\", mayor_covariates)\n",
    "print(\"Municipal Covariates:\", municipal_covariates)\n",
    "print(\"Political and Judicial Covariates:\", political_judicial_covariates)\n",
    "print(\"Dummy Covariates:\", dummy_covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Columns: Index(['first', 'pcorrupt', 'ncorrupt_os', 'valor_corrupt',\n",
      "       'log_valor_corrupt', 'pref_idade_tse', 'pref_masc', 'pref_escola',\n",
      "       'winmargin2000', 'exp_prefeito', 'party_d1', 'party_d3', 'party_d4',\n",
      "       'party_d5', 'party_d6', 'party_d7', 'party_d8', 'party_d9', 'party_d10',\n",
      "       'party_d11', 'party_d12', 'party_d13', 'party_d14', 'party_d15',\n",
      "       'party_d16', 'party_d17', 'party_d18', 'lpop', 'purb', 'p_secundario',\n",
      "       'mun_novo', 'lpib02', 'gini_ipea', 'ENEP2000', 'ENLP2000', 'p_cad_pref',\n",
      "       'sorteio1', 'sorteio2', 'sorteio3', 'sorteio4', 'sorteio5', 'sorteio6',\n",
      "       'sorteio7', 'sorteio8', 'sorteio9', 'sorteio10', 'uf_d1', 'uf_d2',\n",
      "       'uf_d3', 'uf_d4', 'uf_d5', 'uf_d6', 'uf_d7', 'uf_d8', 'uf_d9', 'uf_d10',\n",
      "       'uf_d11', 'uf_d12', 'uf_d13', 'uf_d14', 'uf_d15', 'uf_d16', 'uf_d17',\n",
      "       'uf_d18', 'uf_d19', 'uf_d20', 'uf_d21', 'uf_d22', 'uf_d23', 'uf_d24',\n",
      "       'uf_d25', 'uf_d26'],\n",
      "      dtype='object')\n",
      "Number of rows in the Dataset: 467\n"
     ]
    }
   ],
   "source": [
    "corruption_df[\"log_valor_corrupt\"] = np.log(corruption_df[\"valor_corrupt\"] + 1)\n",
    "\n",
    "# Define the treatment and outcome variables\n",
    "treatment = \"first\"  \n",
    "outcomes = [\"pcorrupt\", \"ncorrupt_os\", \"valor_corrupt\",'log_valor_corrupt'] \n",
    "\n",
    "# Use all covariates\n",
    "all_covariates = (\n",
    "    mayor_covariates +\n",
    "    municipal_covariates +\n",
    "    political_judicial_covariates +\n",
    "    dummy_covariates\n",
    ")\n",
    "#set up dataset:\n",
    "required_columns = [treatment] + outcomes + all_covariates\n",
    "double_ml_dataset = corruption_df[required_columns].dropna()\n",
    "\n",
    "print(\"Dataset Columns:\", double_ml_dataset.columns)\n",
    "print(\"Number of rows in the Dataset:\", double_ml_dataset.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assuming Heterogeneosity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doubly Robust Treatment Effect Estimates with Cross-Fitting:\n",
      "             Outcome       Estimate           MSE\n",
      "0           pcorrupt      -0.022035  1.736679e-02\n",
      "1        ncorrupt_os      -0.008515  3.261818e-03\n",
      "2      valor_corrupt -148571.437500  1.017620e+12\n",
      "3  log_valor_corrupt      -1.067475  3.614077e+01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "theta_hat_dr_values = []\n",
    "mse_values = []\n",
    "\n",
    "# Loop through each outcome\n",
    "for outcome in outcomes:\n",
    "    \n",
    "    X = double_ml_dataset[all_covariates]\n",
    "    D = double_ml_dataset[treatment]\n",
    "    Y = double_ml_dataset[outcome]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Prepare 5-Fold Cross-Fitting\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    dr_ate_fold = []\n",
    "    mse_fold = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "        D_train, D_test = D.iloc[train_index], D.iloc[test_index]\n",
    "        Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "        \n",
    "        # Estimate propensity scores using Lasso\n",
    "        lasso_pscore = LassoCV(alphas=np.logspace(-4, 0, 50), cv=5, random_state=42).fit(X_train, D_train)\n",
    "        D_pred = lasso_pscore.predict(X_test)\n",
    "        propensity_scores = expit(D_pred) \n",
    "        \n",
    "        # Trim p-scores (0.01, 0.99)\n",
    "        trimmed_indices = (propensity_scores > 0.01) & (propensity_scores < 0.99)\n",
    "        trimmed_X = X_test[trimmed_indices]\n",
    "        trimmed_D = D_test.iloc[trimmed_indices]\n",
    "        trimmed_Y = Y_test.iloc[trimmed_indices]\n",
    "        trimmed_pscore = propensity_scores[trimmed_indices]\n",
    "        \n",
    "        # Fit outcome models for treated (D=1) and untreated (D=0) groups\n",
    "        # Treated model\n",
    "        treated_model = LassoCV(alphas=np.logspace(-4, 0, 50), cv=5, random_state=42)\n",
    "        treated_model.fit(X_train[D_train == 1], Y_train[D_train == 1])\n",
    "        gamma1 = treated_model.predict(trimmed_X)\n",
    "        \n",
    "        # Untreated model\n",
    "        untreated_model = LassoCV(alphas=np.logspace(-4, 0, 50), cv=5, random_state=42)\n",
    "        untreated_model.fit(X_train[D_train == 0], Y_train[D_train == 0])\n",
    "        gamma0 = untreated_model.predict(trimmed_X)\n",
    "        \n",
    "        # Step 3: Calculate doubly robust estimates for potential outcomes\n",
    "        trimmed_data = pd.DataFrame({\n",
    "            \"gamma1\": gamma1,\n",
    "            \"gamma0\": gamma0,\n",
    "            \"D\": trimmed_D.values,\n",
    "            \"Y\": trimmed_Y.values,\n",
    "            \"pscore\": trimmed_pscore,\n",
    "        })\n",
    "        \n",
    "        # DR Estimate for Y(1)\n",
    "        trimmed_data['Y1_dr'] = (\n",
    "            trimmed_data['gamma1'] +\n",
    "            (trimmed_data['D'] / trimmed_data['pscore']) * (trimmed_data['Y'] - trimmed_data['gamma1'])\n",
    "        )\n",
    "        \n",
    "        # DR Estimate for Y(0)\n",
    "        trimmed_data['Y0_dr'] = (\n",
    "            trimmed_data['gamma0'] +\n",
    "            ((1 - trimmed_data['D']) / (1 - trimmed_data['pscore'])) * (trimmed_data['Y'] - trimmed_data['gamma0'])\n",
    "        )\n",
    "        \n",
    "        # Calculate treatment effect for the fold\n",
    "        dr_ate_fold.append(np.mean(trimmed_data['Y1_dr'] - trimmed_data['Y0_dr']))\n",
    "        \n",
    "        # Calculate MSE for the fold\n",
    "        mse_fold.append(mean_squared_error(\n",
    "        trimmed_data['Y'], \n",
    "        trimmed_data['Y1_dr'] * trimmed_data['D'] + trimmed_data['Y0_dr'] * (1 - trimmed_data['D'])\n",
    "        ))\n",
    "    \n",
    "    # Average treatment effect and MSE across folds\n",
    "    dr_ate = np.mean(dr_ate_fold)\n",
    "    mse = np.mean(mse_fold)\n",
    "    \n",
    "    # Store results\n",
    "    theta_hat_dr_values.append(dr_ate)\n",
    "    mse_values.append(mse)\n",
    "\n",
    "# Create results DataFrame\n",
    "ls_result_df_hetero = pd.DataFrame({\n",
    "    \"Outcome\": outcomes,\n",
    "    \"Estimate\": theta_hat_dr_values,\n",
    "    \"MSE\": mse_values\n",
    "})\n",
    "\n",
    "# Print the final results DataFrame\n",
    "print(\"Doubly Robust Treatment Effect Estimates with Cross-Fitting:\")\n",
    "print(ls_result_df_hetero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized Mean Differences (SMD): [0.06885722 0.2901101  0.02904402 0.6848417  0.21398759 0.06088972\n",
      " 0.25428903 0.41079307 0.09445282 0.04607635 0.6        0.6\n",
      " 0.35854152 0.24288547 0.24353866 0.03111833 0.16987991 0.2\n",
      " 0.17568141 0.1651254  0.21418196 0.6        0.02996126 0.04102391\n",
      " 0.18319435 0.20693757 0.4547287  0.39264885 0.3478793  0.23060311\n",
      " 0.2800635  0.00521759 0.20551337 0.15774368 0.38865724 0.08364097\n",
      " 0.05980109 0.2552489  0.47479638 0.35756427 0.02450931 0.18689632\n",
      " 0.19421004 0.06287777 0.35430452 0.26188627 0.01593611 0.25\n",
      " 0.46131605 0.3687714  0.05878664 0.0249002  0.26350877 0.01907789\n",
      " 0.1791523  0.16089885 0.17051342 0.3866724  0.17795302 0.25078654\n",
      " 0.23934054 0.625      0.04331093 0.07217446 0.17713386 0.03650402\n",
      " 0.4       ]\n"
     ]
    }
   ],
   "source": [
    "# Checking for balance\n",
    "weights_treated = 1 / propensity_scores[trimmed_D == 1]\n",
    "weights_untreated = 1 / (1 - propensity_scores[trimmed_D == 0])\n",
    "\n",
    "# Create DataFrames for treated and untreated groups\n",
    "treated_data = trimmed_X[trimmed_D == 1]\n",
    "untreated_data = trimmed_X[trimmed_D == 0]\n",
    "\n",
    "# Compute weighted means for each covariate\n",
    "treated_means = np.average(treated_data, axis=0, weights=weights_treated)\n",
    "untreated_means = np.average(untreated_data, axis=0, weights=weights_untreated)\n",
    "\n",
    "# Check absolute standardized mean differences\n",
    "smd = np.abs(treated_means - untreated_means) / np.std(trimmed_X, axis=0)\n",
    "print(\"Standardized Mean Differences (SMD):\", smd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assuming Homogeneous treatment effct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doubly Robust Treatment Effect Estimates with Cross-Fitting (Homogeneous):\n",
      "             Outcome       Estimate           MSE\n",
      "0           pcorrupt      -0.023304  1.026658e-02\n",
      "1        ncorrupt_os      -0.008581  2.078044e-03\n",
      "2      valor_corrupt -139784.148524  3.580824e+11\n",
      "3  log_valor_corrupt      -1.205078  2.475476e+01\n"
     ]
    }
   ],
   "source": [
    "theta_hat_dr_values = []\n",
    "mse_values = []\n",
    "\n",
    "# Loop through each outcome\n",
    "for outcome in outcomes:\n",
    "    \n",
    "    X = double_ml_dataset[all_covariates].values\n",
    "    D = double_ml_dataset[treatment].values\n",
    "    Y = double_ml_dataset[outcome].values\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # K-Fold Cross-Fitting\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    W_hat, V_hat = np.zeros(len(Y)), np.zeros(len(Y))\n",
    "    mse_fold = []  # List to store MSE for each fold\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "        D_train, D_test = D[train_index], D[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "        # Step 1: Estimate E[Y|X] using Lasso (Outcome model)\n",
    "        outcome_model = LassoCV(alphas=np.logspace(-4, 0, 50), cv=5, random_state=42)\n",
    "        outcome_model.fit(X_train, Y_train)\n",
    "        gamma_hat = outcome_model.predict(X_test)\n",
    "\n",
    "        # Step 2: Estimate E[D|X] using Lasso (Propensity score model)\n",
    "        pscore_model = LassoCV(alphas=np.logspace(-4, 0, 50), cv=5, random_state=42)\n",
    "        pscore_model.fit(X_train, D_train)\n",
    "        pi_hat = pscore_model.predict(X_test)\n",
    "        \n",
    "        # Step 3: Calculate residuals W_hat and V_hat\n",
    "        W_hat[test_index] = Y_test - gamma_hat\n",
    "        V_hat[test_index] = D_test - pi_hat\n",
    "\n",
    "        # Step 4: Calculate MSE for this fold\n",
    "        mse_fold.append(mean_squared_error(Y_test, gamma_hat))\n",
    "\n",
    "    # Step 5: Regress W_hat on V_hat to estimate theta_0\n",
    "    regression = LinearRegression()\n",
    "    regression.fit(V_hat.reshape(-1, 1), W_hat)\n",
    "    theta_hat = regression.coef_[0]\n",
    "    \n",
    "    # Store results\n",
    "    theta_hat_dr_values.append(theta_hat)\n",
    "    mse_values.append(np.mean(mse_fold))  # Average MSE across folds\n",
    "\n",
    "# Step 6: Create Results DataFrame\n",
    "ls_result_df_homo = pd.DataFrame({\n",
    "    \"Outcome\": outcomes,\n",
    "    \"Estimate\": theta_hat_dr_values,\n",
    "    \"MSE\": mse_values\n",
    "})\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(\"Doubly Robust Treatment Effect Estimates with Cross-Fitting (Homogeneous):\")\n",
    "print(ls_result_df_homo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assuming Heterogeneous Treatment Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doubly Robust Treatment Effect Estimates with Cross-Fitting:\n",
      "             Outcome       Estimate           MSE\n",
      "0           pcorrupt      -0.026246  1.801923e-02\n",
      "1        ncorrupt_os      -0.007434  2.945456e-03\n",
      "2      valor_corrupt -144203.110003  8.236523e+11\n",
      "3  log_valor_corrupt      -0.910680  3.315613e+01\n"
     ]
    }
   ],
   "source": [
    "theta_hat_dr_values = []\n",
    "mse_values = []\n",
    "\n",
    "# Loop through each outcome\n",
    "for outcome in outcomes:\n",
    "    \n",
    "    X = double_ml_dataset[all_covariates]\n",
    "    D = double_ml_dataset[treatment]\n",
    "    Y = double_ml_dataset[outcome]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Prepare 5-Fold Cross-Fitting\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    dr_ate_fold = []\n",
    "    mse_fold = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "        D_train, D_test = D.iloc[train_index], D.iloc[test_index]\n",
    "        Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "        \n",
    "        # Estimate propensity scores using Random Forest\n",
    "        rf_pscore = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "        rf_pscore.fit(X_train, D_train)\n",
    "        propensity_scores = rf_pscore.predict_proba(X_test)[:, 1]  # Probability of treatment\n",
    "        \n",
    "        # Trim p-scores (0.01, 0.99)\n",
    "        trimmed_indices = (propensity_scores > 0.01) & (propensity_scores < 0.99)\n",
    "        trimmed_X = X_test[trimmed_indices]\n",
    "        trimmed_D = D_test.iloc[trimmed_indices]\n",
    "        trimmed_Y = Y_test.iloc[trimmed_indices]\n",
    "        trimmed_pscore = propensity_scores[trimmed_indices]\n",
    "        \n",
    "        # Fit outcome models for treated (D=1) and untreated (D=0) groups using Random Forest\n",
    "        treated_model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "        treated_model.fit(X_train[D_train == 1], Y_train[D_train == 1])\n",
    "        gamma1 = treated_model.predict(trimmed_X)\n",
    "        \n",
    "        untreated_model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "        untreated_model.fit(X_train[D_train == 0], Y_train[D_train == 0])\n",
    "        gamma0 = untreated_model.predict(trimmed_X)\n",
    "        \n",
    "        trimmed_data = pd.DataFrame({\n",
    "            \"gamma1\": gamma1,\n",
    "            \"gamma0\": gamma0,\n",
    "            \"D\": trimmed_D.values,\n",
    "            \"Y\": trimmed_Y.values,\n",
    "            \"pscore\": trimmed_pscore,\n",
    "        })\n",
    "        \n",
    "        # DR Estimate for Y(1)\n",
    "        trimmed_data['Y1_dr'] = (\n",
    "            trimmed_data['gamma1'] +\n",
    "            (trimmed_data['D'] / trimmed_data['pscore']) * (trimmed_data['Y'] - trimmed_data['gamma1'])\n",
    "        )\n",
    "        \n",
    "        # DR Estimate for Y(0)\n",
    "        trimmed_data['Y0_dr'] = (\n",
    "            trimmed_data['gamma0'] +\n",
    "            ((1 - trimmed_data['D']) / (1 - trimmed_data['pscore'])) * (trimmed_data['Y'] - trimmed_data['gamma0'])\n",
    "        )\n",
    "        \n",
    "        # Calculate treatment effect and MSE for the fold\n",
    "        dr_ate_fold.append(np.mean(trimmed_data['Y1_dr'] - trimmed_data['Y0_dr']))\n",
    "        mse_fold.append(mean_squared_error(\n",
    "            trimmed_data['Y'], \n",
    "            trimmed_data['Y1_dr'] * trimmed_data['D'] + trimmed_data['Y0_dr'] * (1 - trimmed_data['D'])\n",
    "        ))\n",
    "    \n",
    "    # Average treatment effect and MSE across folds\n",
    "    dr_ate = np.mean(dr_ate_fold)\n",
    "    mse = np.mean(mse_fold)\n",
    "    \n",
    "    theta_hat_dr_values.append(dr_ate)\n",
    "    mse_values.append(mse)\n",
    "\n",
    "# Create results DataFrame\n",
    "rf_result_df_hetero = pd.DataFrame({\n",
    "    \"Outcome\": outcomes,\n",
    "    \"Estimate\": theta_hat_dr_values,\n",
    "    \"MSE\": mse_values\n",
    "})\n",
    "\n",
    "# Print the final results\n",
    "print(\"Doubly Robust Treatment Effect Estimates with Cross-Fitting:\")\n",
    "print(rf_result_df_hetero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized Mean Differences (SMD): [6.50356092e-02 2.62891458e-01 4.46274547e-02 5.68645557e-01\n",
      " 1.40846193e-01 2.85167214e-02 2.39017798e-01 4.21856346e-01\n",
      " 1.70203834e-01 3.12581267e-02 1.11758709e-09 1.11758709e-09\n",
      " 3.40814162e-01 2.19392841e-01 2.04254184e-01 6.68983541e-04\n",
      " 1.36899855e-01 1.11758709e-09 1.48401816e-01 1.57523424e-01\n",
      " 1.77873843e-01 1.11758709e-09 1.64725845e-01 1.52697392e-01\n",
      " 9.21535698e-03 2.80392638e-01 3.54954060e-01 4.13530648e-01\n",
      " 1.65639872e-01 5.90287328e-02 1.05886511e-01 7.97249442e-03\n",
      " 9.18892102e-02 1.52543983e-01 4.31016975e-01 9.91344798e-02\n",
      " 1.11597675e-01 2.86269278e-01 4.33117663e-01 4.24619704e-01\n",
      " 2.35066993e-02 1.68955984e-01 1.73020941e-01 1.04110585e-01\n",
      " 3.15033490e-01 2.44864719e-01 7.19494572e-02 4.65661287e-10\n",
      " 4.64575176e-01 3.91367717e-01 2.11395745e-02 8.00353031e-03\n",
      " 2.68994830e-01 1.66763040e-03 1.63270268e-01 1.08851813e-01\n",
      " 8.79607502e-02 4.67152370e-01 2.46684250e-01 2.45766903e-01\n",
      " 1.55956523e-01 2.32830644e-10 2.61510988e-02 8.38009898e-02\n",
      " 1.90309951e-01 1.61492627e-02 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Checking for balance\n",
    "weights_treated = 1 / propensity_scores[trimmed_D == 1]\n",
    "weights_untreated = 1 / (1 - propensity_scores[trimmed_D == 0])\n",
    "\n",
    "# Create DataFrames for treated and untreated groups\n",
    "treated_data = trimmed_X[trimmed_D == 1]\n",
    "untreated_data = trimmed_X[trimmed_D == 0]\n",
    "\n",
    "# Compute weighted means for each covariate\n",
    "treated_means = np.average(treated_data, axis=0, weights=weights_treated)\n",
    "untreated_means = np.average(untreated_data, axis=0, weights=weights_untreated)\n",
    "\n",
    "# Check absolute standardized mean differences\n",
    "smd = np.abs(treated_means - untreated_means) / np.std(trimmed_X, axis=0)\n",
    "print(\"Standardized Mean Differences (SMD):\", smd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assuming Homogeneous Treatment Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doubly Robust Treatment Effect Estimates with Cross-Fitting (Homogeneous) using Random Forest:\n",
      "             Outcome       Estimate           MSE\n",
      "0           pcorrupt      -0.024575  1.088266e-02\n",
      "1        ncorrupt_os      -0.007833  2.269427e-03\n",
      "2      valor_corrupt -132970.875697  3.256660e+11\n",
      "3  log_valor_corrupt      -0.966588  2.605650e+01\n"
     ]
    }
   ],
   "source": [
    "theta_hat_dr_values = []\n",
    "mse_values = []\n",
    "\n",
    "# Loop through each outcome\n",
    "for outcome in outcomes:\n",
    "    \n",
    "    X = double_ml_dataset[all_covariates].values\n",
    "    D = double_ml_dataset[treatment].values\n",
    "    Y = double_ml_dataset[outcome].values\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # K-Fold Cross-Fitting\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    W_hat, V_hat = np.zeros(len(Y)), np.zeros(len(Y))\n",
    "    mse_fold = []  # List to store MSE for each fold\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "        D_train, D_test = D[train_index], D[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "        # Step 1: Estimate E[Y|X] using Random Forest (Outcome model)\n",
    "        outcome_model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "        outcome_model.fit(X_train, Y_train)\n",
    "        gamma_hat = outcome_model.predict(X_test)\n",
    "\n",
    "        # Step 2: Estimate E[D|X] using Random Forest (Propensity score model)\n",
    "        pscore_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "        pscore_model.fit(X_train, D_train)\n",
    "        pi_hat = pscore_model.predict_proba(X_test)[:, 1]  # Probability of treatment\n",
    "        \n",
    "        # Step 3: Calculate residuals W_hat and V_hat\n",
    "        W_hat[test_index] = Y_test - gamma_hat\n",
    "        V_hat[test_index] = D_test - pi_hat\n",
    "\n",
    "        # Step 4: Calculate MSE for this fold\n",
    "        mse_fold.append(mean_squared_error(Y_test, gamma_hat))\n",
    "\n",
    "    # Step 5: Regress W_hat on V_hat to estimate theta_0\n",
    "    regression = LinearRegression()\n",
    "    regression.fit(V_hat.reshape(-1, 1), W_hat)\n",
    "    theta_hat = regression.coef_[0]\n",
    "    \n",
    "    # Store results\n",
    "    theta_hat_dr_values.append(theta_hat)\n",
    "    mse_values.append(np.mean(mse_fold))  # Average MSE across folds\n",
    "\n",
    "# Step 6: Create Results DataFrame\n",
    "rf_result_df_homo = pd.DataFrame({\n",
    "    \"Outcome\": outcomes,\n",
    "    \"Estimate\": theta_hat_dr_values,\n",
    "    \"MSE\": mse_values\n",
    "})\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(\"Doubly Robust Treatment Effect Estimates with Cross-Fitting (Homogeneous) using Random Forest:\")\n",
    "print(rf_result_df_homo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build and train an MLP model\n",
    "def train_mlp(X_train, Y_train, input_dim, output_activation='linear', loss='mse', verbose=0):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation=output_activation)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=['mae'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(X_train, Y_train, validation_split=0.2, epochs=100, batch_size=32,\n",
    "              callbacks=[early_stopping], verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate doubly robust treatment effects using MLP\n",
    "def double_ml_mlp_heterogeneous(X, D, Y, outcomes):\n",
    "    theta_hat_dr_values = []\n",
    "    mse_values = []\n",
    "\n",
    "    # Standardize covariates\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 5-Fold Cross-Fitting\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for outcome in outcomes:\n",
    "        dr_ate_fold = []\n",
    "        mse_fold = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(X_scaled):\n",
    "            X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "            D_train, D_test = D[train_index], D[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "            \n",
    "            # Step 1: Propensity score estimation using MLP\n",
    "            model_pscore = train_mlp(X_train, D_train, input_dim=X_train.shape[1], \n",
    "                                     output_activation='sigmoid', loss='binary_crossentropy')\n",
    "            propensity_scores = model_pscore.predict(X_test).flatten()\n",
    "\n",
    "            # Trim p-scores (0.01, 0.99)\n",
    "            trimmed_indices = (propensity_scores > 0.01) & (propensity_scores < 0.99)\n",
    "            trimmed_X = X_test[trimmed_indices]\n",
    "            trimmed_D = D_test[trimmed_indices]\n",
    "            trimmed_Y = Y_test[trimmed_indices]\n",
    "            trimmed_pscore = propensity_scores[trimmed_indices]\n",
    "\n",
    "            # Step 2: Fit outcome models for treated and untreated groups\n",
    "            model_treated = train_mlp(X_train[D_train == 1], Y_train[D_train == 1], input_dim=X_train.shape[1])\n",
    "            gamma1 = model_treated.predict(trimmed_X).flatten()\n",
    "\n",
    "            model_untreated = train_mlp(X_train[D_train == 0], Y_train[D_train == 0], input_dim=X_train.shape[1])\n",
    "            gamma0 = model_untreated.predict(trimmed_X).flatten()\n",
    "\n",
    "            # Step 3: Construct doubly robust estimates\n",
    "            trimmed_data = pd.DataFrame({\n",
    "                \"gamma1\": gamma1,\n",
    "                \"gamma0\": gamma0,\n",
    "                \"D\": trimmed_D,\n",
    "                \"Y\": trimmed_Y,\n",
    "                \"pscore\": trimmed_pscore\n",
    "            })\n",
    "            trimmed_data['Y1_dr'] = (\n",
    "                trimmed_data['gamma1'] +\n",
    "                (trimmed_data['D'] / trimmed_data['pscore']) * (trimmed_data['Y'] - trimmed_data['gamma1'])\n",
    "            )\n",
    "            trimmed_data['Y0_dr'] = (\n",
    "                trimmed_data['gamma0'] +\n",
    "                ((1 - trimmed_data['D']) / (1 - trimmed_data['pscore'])) * (trimmed_data['Y'] - trimmed_data['gamma0'])\n",
    "            )\n",
    "\n",
    "            # Calculate treatment effect and MSE for the fold\n",
    "            dr_ate_fold.append(np.mean(trimmed_data['Y1_dr'] - trimmed_data['Y0_dr']))\n",
    "            mse_fold.append(mean_squared_error(\n",
    "                trimmed_data['Y'], \n",
    "                trimmed_data['Y1_dr'] * trimmed_data['D'] + trimmed_data['Y0_dr'] * (1 - trimmed_data['D'])\n",
    "            ))\n",
    "\n",
    "        # Store average estimates and MSE\n",
    "        theta_hat_dr_values.append(np.mean(dr_ate_fold))\n",
    "        mse_values.append(np.mean(mse_fold))\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        \"Outcome\": outcomes,\n",
    "        \"Estimate\": theta_hat_dr_values,\n",
    "        \"MSE\": mse_values\n",
    "    })\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for easy call \n",
    "def run_double_ml_mlp(dataset, covariants, outcomes):\n",
    "    X = dataset[covariants].values\n",
    "    D = dataset[treatment].values\n",
    "    results = []\n",
    "    \n",
    "    for outcome in outcomes:\n",
    "        Y = dataset[outcome].values\n",
    "        print(f\"Processing outcome: {outcome}\")\n",
    "        result = double_ml_mlp_heterogeneous(X, D, Y, [outcome])\n",
    "        results.append(result)\n",
    "    \n",
    "    final_results = pd.concat(results, ignore_index=True)\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing outcome: pcorrupt\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "Processing outcome: ncorrupt_os\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "Processing outcome: valor_corrupt\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "Processing outcome: log_valor_corrupt\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "Doubly Robust Treatment Effect Estimates with Cross-Fitting (Heterogeneous) using MLP:\n",
      "             Outcome       Estimate           MSE\n",
      "0           pcorrupt      -0.016243  1.629759e-01\n",
      "1        ncorrupt_os       0.003273  6.135722e-02\n",
      "2      valor_corrupt -146446.062500  8.334396e+11\n",
      "3  log_valor_corrupt      -0.764382  4.168226e+01\n"
     ]
    }
   ],
   "source": [
    "#mlp_result with all covariants\n",
    "mlp_result_df_hetero = run_double_ml_mlp(double_ml_dataset, all_covariates, outcomes)\n",
    "print(\"Doubly Robust Treatment Effect Estimates with Cross-Fitting (Heterogeneous) using MLP:\")\n",
    "mlp_result_df_hetero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized Mean Differences (SMD): [0.06431737 0.17436421 0.08196042 0.42424825 0.2740144  0.05741303\n",
      " 0.26795167 0.24450615 0.15918534 0.37349612 0.6        0.6\n",
      " 0.28162652 0.24226102 0.18890478 0.16642605 0.11068814 0.2\n",
      " 0.11865247 0.13747789 0.21630324 0.6        0.3615755  0.13864243\n",
      " 0.1238498  0.25766122 0.26216054 0.34757185 0.14859101 0.13102074\n",
      " 0.12994067 0.02177119 0.07538263 0.18149064 0.41006213 0.1398001\n",
      " 0.15731958 0.28548157 0.4361691  0.5003572  0.08816264 0.13607545\n",
      " 0.11496648 0.4749314  0.27814373 0.24964912 0.00266549 0.25\n",
      " 0.44307894 0.3509568  0.01910833 0.05646941 0.28625125 0.08249972\n",
      " 0.05899588 0.20087045 0.1198876  0.38367516 0.2754754  0.27015147\n",
      " 0.13837029 0.5        0.21252668 0.14661519 0.31280857 0.28590313\n",
      " 0.1       ]\n"
     ]
    }
   ],
   "source": [
    "# Checking for balance\n",
    "weights_treated = 1 / propensity_scores[trimmed_D == 1]\n",
    "weights_untreated = 1 / (1 - propensity_scores[trimmed_D == 0])\n",
    "\n",
    "# Create DataFrames for treated and untreated groups\n",
    "treated_data = trimmed_X[trimmed_D == 1]\n",
    "untreated_data = trimmed_X[trimmed_D == 0]\n",
    "\n",
    "# Compute weighted means for each covariate\n",
    "treated_means = np.average(treated_data, axis=0, weights=weights_treated)\n",
    "untreated_means = np.average(untreated_data, axis=0, weights=weights_untreated)\n",
    "\n",
    "# Check absolute standardized mean differences\n",
    "smd = np.abs(treated_means - untreated_means) / np.std(trimmed_X, axis=0)\n",
    "print(\"Standardized Mean Differences (SMD):\", smd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assuming Homogeneity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(X_train, Y_train, input_dim, output_activation='linear', loss='mse', verbose=0):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation=output_activation)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=['mae'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(X_train, Y_train, validation_split=0.2, epochs=100, batch_size=32,\n",
    "              callbacks=[early_stopping], verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Double ML for a single outcome\n",
    "def double_ml_single_outcome_homogeneous(X, D, Y):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Prepare for K-Fold Cross-Fitting\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    W_hat, V_hat = np.zeros(len(Y)), np.zeros(len(Y))\n",
    "    mse_fold = []  # List to store MSE for each fold\n",
    "\n",
    "    for train_index, test_index in kf.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "        D_train, D_test = D[train_index], D[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "        # Step 1: Estimate E[Y|X] using MLP (Outcome model)\n",
    "        outcome_model = train_mlp(X_train, Y_train, input_dim=X_train.shape[1])\n",
    "        gamma_hat = outcome_model.predict(X_test).flatten()\n",
    "\n",
    "        # Step 2: Estimate E[D|X] using MLP (Propensity score model)\n",
    "        pscore_model = train_mlp(X_train, D_train, input_dim=X_train.shape[1], \n",
    "                                 output_activation='sigmoid', loss='binary_crossentropy')\n",
    "        pi_hat = pscore_model.predict(X_test).flatten()\n",
    "\n",
    "        # Step 3: Calculate residuals W_hat and V_hat\n",
    "        W_hat[test_index] = Y_test - gamma_hat\n",
    "        V_hat[test_index] = D_test - pi_hat\n",
    "\n",
    "        # Step 4: Calculate MSE for this fold\n",
    "        mse_fold.append(mean_squared_error(Y_test, gamma_hat))\n",
    "\n",
    "    # Step 5: Regress W_hat on V_hat to estimate theta_0\n",
    "    regression = LinearRegression()\n",
    "    regression.fit(V_hat.reshape(-1, 1), W_hat)\n",
    "    theta_hat = regression.coef_[0]\n",
    "    \n",
    "    return theta_hat, np.mean(mse_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to loop through multiple outcomes\n",
    "def run_double_ml_mlp_homogeneous(dataset, covariates, treatment, outcomes):\n",
    "    theta_hat_dr_values = []\n",
    "    mse_values = []\n",
    "\n",
    "    for outcome in outcomes:\n",
    "        print(f\"Processing outcome: {outcome}\")\n",
    "        \n",
    "        X = dataset[covariates].values\n",
    "        D = dataset[treatment].values\n",
    "        Y = dataset[outcome].values\n",
    "\n",
    "        # Perform Double ML for the single outcome\n",
    "        theta_hat, mse = double_ml_single_outcome_homogeneous(X, D, Y)\n",
    "        \n",
    "        theta_hat_dr_values.append(theta_hat)\n",
    "        mse_values.append(mse)\n",
    "\n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        \"Outcome\": outcomes,\n",
    "        \"Estimate\": theta_hat_dr_values,\n",
    "        \"MSE\": mse_values\n",
    "    })\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing outcome: pcorrupt\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "Processing outcome: ncorrupt_os\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "Processing outcome: valor_corrupt\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "Processing outcome: log_valor_corrupt\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "mlp_result_df_homo = run_double_ml_mlp_homogeneous(\n",
    "    dataset=double_ml_dataset, \n",
    "    covariates=all_covariates, \n",
    "    treatment=treatment, \n",
    "    outcomes=outcomes\n",
    ")\n",
    "mlp_result_df_homo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outcome</th>\n",
       "      <th>Estimate</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pcorrupt</td>\n",
       "      <td>-0.014981</td>\n",
       "      <td>3.877918e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ncorrupt_os</td>\n",
       "      <td>0.012934</td>\n",
       "      <td>2.207904e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>valor_corrupt</td>\n",
       "      <td>-123446.626703</td>\n",
       "      <td>3.299329e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>log_valor_corrupt</td>\n",
       "      <td>-0.744095</td>\n",
       "      <td>3.068615e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Outcome       Estimate           MSE\n",
       "0           pcorrupt      -0.014981  3.877918e-02\n",
       "1        ncorrupt_os       0.012934  2.207904e-02\n",
       "2      valor_corrupt -123446.626703  3.299329e+11\n",
       "3  log_valor_corrupt      -0.744095  3.068615e+01"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_result_df_homo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assuming Heteogeneous treatment effct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train propensity score model using Gradient Boosting\n",
    "def train_gb_pscore(X_train, D_train, X_test, n_estimators=100, max_depth=3, random_state=42):\n",
    "    gb_pscore = GradientBoostingClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n",
    "    gb_pscore.fit(X_train, D_train)\n",
    "    propensity_scores = gb_pscore.predict_proba(X_test)[:, 1]\n",
    "    return propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train outcome model for treated/untreated groups\n",
    "def train_gb_outcome(X_train, Y_train, X_test, n_estimators=100, max_depth=3, random_state=42):\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n",
    "    gb_model.fit(X_train, Y_train)\n",
    "    predictions = gb_model.predict(X_test)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the doubly robust treatment effect\n",
    "def calculate_dr_treatment_effect(X, D, Y, k_folds=5):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # K-Fold Cross-Fitting\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    dr_ate_fold = []\n",
    "    mse_fold = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "        D_train, D_test = D.iloc[train_index], D.iloc[test_index]\n",
    "        Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "        \n",
    "        # Step 1: Estimate propensity scores\n",
    "        propensity_scores = train_gb_pscore(X_train, D_train, X_test)\n",
    "        \n",
    "        # Step 2: Trim p-scores (0.01, 0.99)\n",
    "        trimmed_indices = (propensity_scores > 0.01) & (propensity_scores < 0.99)\n",
    "        trimmed_X = X_test[trimmed_indices]\n",
    "        trimmed_D = D_test.iloc[trimmed_indices]\n",
    "        trimmed_Y = Y_test.iloc[trimmed_indices]\n",
    "        trimmed_pscore = propensity_scores[trimmed_indices]\n",
    "        \n",
    "        # Step 3: Fit outcome models for treated and untreated groups\n",
    "        gamma1 = train_gb_outcome(X_train[D_train == 1], Y_train[D_train == 1], trimmed_X)\n",
    "        gamma0 = train_gb_outcome(X_train[D_train == 0], Y_train[D_train == 0], trimmed_X)\n",
    "        \n",
    "        # Step 4: Calculate doubly robust estimates\n",
    "        trimmed_data = pd.DataFrame({\n",
    "            \"gamma1\": gamma1,\n",
    "            \"gamma0\": gamma0,\n",
    "            \"D\": trimmed_D.values,\n",
    "            \"Y\": trimmed_Y.values,\n",
    "            \"pscore\": trimmed_pscore\n",
    "        })\n",
    "        \n",
    "        trimmed_data['Y1_dr'] = (\n",
    "            trimmed_data['gamma1'] +\n",
    "            (trimmed_data['D'] / trimmed_data['pscore']) * (trimmed_data['Y'] - trimmed_data['gamma1'])\n",
    "        )\n",
    "        \n",
    "        trimmed_data['Y0_dr'] = (\n",
    "            trimmed_data['gamma0'] +\n",
    "            ((1 - trimmed_data['D']) / (1 - trimmed_data['pscore'])) * (trimmed_data['Y'] - trimmed_data['gamma0'])\n",
    "        )\n",
    "        \n",
    "        # Step 5: Calculate treatment effect and MSE for the fold\n",
    "        dr_ate_fold.append(np.mean(trimmed_data['Y1_dr'] - trimmed_data['Y0_dr']))\n",
    "        mse_fold.append(mean_squared_error(\n",
    "            trimmed_data['Y'],\n",
    "            trimmed_data['Y1_dr'] * trimmed_data['D'] + trimmed_data['Y0_dr'] * (1 - trimmed_data['D'])\n",
    "        ))\n",
    "    \n",
    "    # Average treatment effect and MSE across folds\n",
    "    dr_ate = np.mean(dr_ate_fold)\n",
    "    mse = np.mean(mse_fold)\n",
    "    return dr_ate, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_boosting_dml(dataset, covariates, treatment, outcomes):\n",
    "    theta_hat_dr_values = []\n",
    "    mse_values = []\n",
    "\n",
    "    for outcome in outcomes:\n",
    "        print(f\"Processing outcome: {outcome}\")\n",
    "        X = dataset[covariates]\n",
    "        D = dataset[treatment]\n",
    "        Y = dataset[outcome]\n",
    "\n",
    "        dr_ate, mse = calculate_dr_treatment_effect(X, D, Y)\n",
    "        theta_hat_dr_values.append(dr_ate)\n",
    "        mse_values.append(mse)\n",
    "    \n",
    "    # Create Results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        \"Outcome\": outcomes,\n",
    "        \"Estimate\": theta_hat_dr_values,\n",
    "        \"MSE\": mse_values\n",
    "    })\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing outcome: pcorrupt\n",
      "Processing outcome: ncorrupt_os\n",
      "Processing outcome: valor_corrupt\n",
      "Processing outcome: log_valor_corrupt\n"
     ]
    }
   ],
   "source": [
    "gb_result_df_hetero = run_gradient_boosting_dml(\n",
    "    dataset=double_ml_dataset, \n",
    "    covariates=all_covariates, \n",
    "    treatment=treatment, \n",
    "    outcomes=outcomes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doubly Robust Treatment Effect Estimates with Cross-Fitting (Gradient Boosting):\n",
      "             Outcome       Estimate           MSE\n",
      "0           pcorrupt      -0.025620  1.416204e-01\n",
      "1        ncorrupt_os      -0.006851  2.309279e-02\n",
      "2      valor_corrupt -100519.402595  3.362692e+12\n",
      "3  log_valor_corrupt      -0.247871  3.027244e+02\n"
     ]
    }
   ],
   "source": [
    "print(\"Doubly Robust Treatment Effect Estimates with Cross-Fitting (Gradient Boosting):\")\n",
    "print(gb_result_df_hetero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized Mean Differences (SMD): [0.06431737 0.17436421 0.08196042 0.42424825 0.2740144  0.05741303\n",
      " 0.26795167 0.24450615 0.15918534 0.37349612 0.6        0.6\n",
      " 0.28162652 0.24226102 0.18890478 0.16642605 0.11068814 0.2\n",
      " 0.11865247 0.13747789 0.21630324 0.6        0.3615755  0.13864243\n",
      " 0.1238498  0.25766122 0.26216054 0.34757185 0.14859101 0.13102074\n",
      " 0.12994067 0.02177119 0.07538263 0.18149064 0.41006213 0.1398001\n",
      " 0.15731958 0.28548157 0.4361691  0.5003572  0.08816264 0.13607545\n",
      " 0.11496648 0.4749314  0.27814373 0.24964912 0.00266549 0.25\n",
      " 0.44307894 0.3509568  0.01910833 0.05646941 0.28625125 0.08249972\n",
      " 0.05899588 0.20087045 0.1198876  0.38367516 0.2754754  0.27015147\n",
      " 0.13837029 0.5        0.21252668 0.14661519 0.31280857 0.28590313\n",
      " 0.1       ]\n"
     ]
    }
   ],
   "source": [
    "# Checking for balance\n",
    "weights_treated = 1 / propensity_scores[trimmed_D == 1]\n",
    "weights_untreated = 1 / (1 - propensity_scores[trimmed_D == 0])\n",
    "\n",
    "# Create DataFrames for treated and untreated groups\n",
    "treated_data = trimmed_X[trimmed_D == 1]\n",
    "untreated_data = trimmed_X[trimmed_D == 0]\n",
    "\n",
    "# Compute weighted means for each covariate\n",
    "treated_means = np.average(treated_data, axis=0, weights=weights_treated)\n",
    "untreated_means = np.average(untreated_data, axis=0, weights=weights_untreated)\n",
    "\n",
    "# Check absolute standardized mean differences\n",
    "smd = np.abs(treated_means - untreated_means) / np.std(trimmed_X, axis=0)\n",
    "print(\"Standardized Mean Differences (SMD):\", smd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assuming Homogeneous treatment effct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dr_homogeneous_gb(X, D, Y, k_folds=5):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # K-Fold Cross-Fitting\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    W_hat, V_hat = np.zeros(len(Y)), np.zeros(len(Y))\n",
    "    mse_fold = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "        D_train, D_test = D[train_index], D[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "        # Step 1: Estimate propensity scores using the provided function\n",
    "        pi_hat = train_gb_pscore(X_train, D_train, X_test)  # Function from the image\n",
    "\n",
    "        # Trim propensity scores (0.01, 0.99)\n",
    "        trimmed_indices = (pi_hat > 0.01) & (pi_hat < 0.99)\n",
    "        trimmed_X, trimmed_D, trimmed_Y = X_test[trimmed_indices], D_test[trimmed_indices], Y_test[trimmed_indices]\n",
    "        trimmed_pscore = pi_hat[trimmed_indices]\n",
    "\n",
    "        # Step 2: Estimate E[Y|X] using the provided function\n",
    "        gamma_hat = train_gb_outcome(X_train, Y_train, trimmed_X)  # Function from the image\n",
    "\n",
    "        # Step 3: Calculate residuals W_hat and V_hat\n",
    "        W_hat[test_index[trimmed_indices]] = trimmed_Y - gamma_hat\n",
    "        V_hat[test_index[trimmed_indices]] = trimmed_D - trimmed_pscore\n",
    "\n",
    "        # Step 4: Calculate MSE for this fold\n",
    "        mse_fold.append(mean_squared_error(trimmed_Y, gamma_hat))\n",
    "    \n",
    "    # Step 5: Regress W_hat on V_hat to estimate theta_0\n",
    "    regression = LinearRegression()\n",
    "    regression.fit(V_hat.reshape(-1, 1), W_hat)\n",
    "    theta_hat = regression.coef_[0]\n",
    "\n",
    "    return theta_hat, np.mean(mse_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dr_homogeneous_gb(dataset, covariates, treatment, outcomes):\n",
    "    theta_hat_dr_values = []\n",
    "    mse_values = []\n",
    "\n",
    "    for outcome in outcomes:\n",
    "        print(f\"Processing outcome: {outcome}\")\n",
    "        X = dataset[covariates].values\n",
    "        D = dataset[treatment].values\n",
    "        Y = dataset[outcome].values\n",
    "\n",
    "        # Call the homogeneous treatment effect function\n",
    "        dr_ate, mse = calculate_dr_homogeneous_gb(X, D, Y)\n",
    "        theta_hat_dr_values.append(dr_ate)\n",
    "        mse_values.append(mse)\n",
    "\n",
    "    # Create Results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        \"Outcome\": outcomes,\n",
    "        \"Estimate\": theta_hat_dr_values,\n",
    "        \"MSE\": mse_values\n",
    "    })\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing outcome: pcorrupt\n",
      "Processing outcome: ncorrupt_os\n",
      "Processing outcome: valor_corrupt\n",
      "Processing outcome: log_valor_corrupt\n"
     ]
    }
   ],
   "source": [
    "gb_result_df_homo = run_dr_homogeneous_gb(\n",
    "    dataset=double_ml_dataset, \n",
    "    covariates=all_covariates, \n",
    "    treatment=treatment, \n",
    "    outcomes=outcomes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doubly Robust Treatment Effect Estimates with Cross-Fitting (Homogeneous, Gradient Boosting):\n",
      "             Outcome      Estimate           MSE\n",
      "0           pcorrupt     -0.018570  1.225573e-02\n",
      "1        ncorrupt_os     -0.006758  2.253265e-03\n",
      "2      valor_corrupt -94125.137029  3.690273e+11\n",
      "3  log_valor_corrupt     -0.805899  2.877532e+01\n"
     ]
    }
   ],
   "source": [
    "print(\"Doubly Robust Treatment Effect Estimates with Cross-Fitting (Homogeneous, Gradient Boosting):\")\n",
    "print(gb_result_df_homo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis \n",
    "\n",
    "compare mse/theta,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info2950",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
